![leafSun](https://user-images.githubusercontent.com/32124230/107999911-9f3f1100-6fe0-11eb-9755-f48dbb82cf65.png)

# DOCUMENTATION

# Table of Contents
* [Background](#background)
  * [Climate projections](#climate-projections)
  * [Crop data](#crop-data)
  * [Example data](#example-data)
* [Basic operation](#basic-operation)
* [Model Tab](#model-tab)
  * [Modelling task](#modelling-task)
  * [Required data format and upload](#required-data-format-and-upload)
  * [Setting a seed](#setting-a-seed)
  * [Parallel computing](#parallel-computing)
  * [Fitting an algorithm](#fitting-an-algorithm)
    * [Model evaluation](#model-evaluation)
    * [Results table](#results-table)
    * [Final model version](#final-model-version)
  * [Saving model results](#saving-model-results)
* [Projections tab](#projections-tab)
  * [Climate and crop panel](#climate-and-crop-panel)
  * [Results panel](#results-panel)
  * [Saving projection results](#saving-projection-results)
  
# Background
The app provides a very simple front-end to cutting edge machine learning algorithms and techniques, and is bundled with gridded crop distribution and climate change data. With a few clicks a weather-dependent regression- or classification-tree ensemble can be fitted to data and applied across real crop locations to explore possible future changes under various climate change scenarios. 

## Climate projections
The app uses seasonal 12 km gridded (65 x 112 cells) climate data from the UK Met Office Climate Projections database [UKCP18](https://www.metoffice.gov.uk/research/approach/collaboration/ukcp/index) 12-member ensemble of Regional projections. The following variables are provided: temperature (deg. C), relative humidity (%), wind speed (m s<sup>-1</sup>), net surface shortwave radiation flux (W m<sup>-2</sup>), precipitation (mm d<sup>-1</sup>), total cloud (%). The data are at presented at seasonal timescales (spring, summer, autumn, and winter) for each decade spanning 2020s-2060s. UKCP18 modelled data for 1981-2000 are included as baseline values for comparison. All the climate data have full spatial and temporal coherence, enabling the assessment of multiple climatic drivers of changing hazards across multiple geographic locations. The data provide 12 equally plausible snapshots of climate change for emissions scenario RCP8.5, meaning that fitted models will produce an ensemble of 12 projected values for each grid cell, and a super-ensemble of results for each crop species distribution.

## Crop data
Polygon data defining the spatial coverage of crops and land-use types were derived from [IACS](https://ec.europa.eu/agriculture/direct-support/iacs_en) and [JACS](https://www.gov.scot/collections/june-scottish-agricultural-census/). These data cover Scotland only. The vector data were rasterised to 12 km grids matching the resolution of the climate change data. Information on the abundance (area) of crop in each grid cell is used for visualisation purposes only, and the presence-absence of selected crop species is used to narrow the climate change risk assessment to relevant grid cells. 

## Example data
Two synthetic datasets (classificationExample.xlsx, regressionExample.xlsx) have been included to allow users to investigate the various features of the app. For best results, fit a model in the Model tab and apply it in barley crop distributions using summer weather data. 

## Basic operation
*Tabs*: The app has two 'tabs' - 'Model' and 'Projections.' Click on the tab name to switch from one to the other.  
*Switches*: Drag the circular switch to the left or right, or click in the empty space.  
*Lists*: Click on the desired option in the list.  
*Buttons*: Click them.  
*Numeric fields*: Click in the white box to change the numerical value, then hit enter or click outside the box.  
*State buttons*: Click them to toggle between two states - dark grey indicates 'on' and light grey is 'off'.  
*Knobs*: Drag the circular control around to your selection, or click on your selection.  
Note: some controls may be unavailable (greyed out) until certain operations have been performed, for example, model results cannot be saved until a model has been fitted to your data.  

# Model Tab
This is a simple front-end that allows non-modellers to easily access cutting-edge machine learning algorithms and fit them to data to create a risk model that can be applied in the [Projections tab](#projections-tab) to perform climate change risk assessments. User data must be a function of one or more of the weather variables included in the [Climate projection](#climate-projections) datasets. Inclusion of other weather variables in your datafile will produce meaningless results. The 'response variable' (dependent variable) can be any weather-dependent process involving / affecting / occurring in or around crops, e.g., crop diseases, pests, crop growth etc.    

## Modelling task
Use the 'Task' switch to choose between regression and classification tasks. The app uses the MATLAB commands ``fitrensemble`` and ``fitcensemble`` to fit ensembles of regression trees and classification trees. The specific ensemble aggregation algorithms used are : ``LSBoost`` for regression, ``AdaBoostM1`` for binary classification problems, ``AdaBoostM2`` for multi-class problems, and ``RUSBoost`` for any classification problem with imbalanced data. A description of these algorithms is beyond the scope of this documentation, but the interested reader can consult the [MATLAB documentation](https://www.mathworks.com/help/index.html?s_tid=CRUX_topnav). For regression tasks the Y variable should be continuous and for classification tasks it must be categorical. If an attempt is made to upload data that do not match the selected task, e.g., classification is selected but data with a continuous Y variable is uploaded, a warning will be issued and the data will not be loaded. Changing the model task will reset the app and erase all uploaded data and results.

## Required data format and upload
To upload data for model fitting click the 'Load' button. This will open up a dialogue box that allows you to search for your file. Data files must be .xls or .xlsx. Data should be arranged in rows with columns for the different weather variables. The column header for the response (dependent) variable should be an upper case Y and the column headers for your weather variables should be all lower case: temp, rh, rain, wind, rad, cloud. It doesn't matter what order the columns are in, and you can include as many of the set of 6 weather variables as required. Missing data should be left as blank cells and these will be ignored during model fitting; do not use numerical missing data identifiers (e.g., -9999) as these will be treated as numbers. Text entries will be treated as missing data. Data for classification tasks require a little more consideration. The categorical response variable must be coded as integers and not any form of character or string array (text, alphanumeric, symbols etc.), e.g., low, medium, and high should be converted to 1, 2, 3. Integers will be treated as discrete classes by the algorithms. This is required to eliminate the risk of typos, use of different date formats etc. You ideally want >25 values of Y for each discrete class. An acceptable performance may be achieved with fewer samples per class, but <5 per class on average will result in a warning and the data will not load. Consider aggregating the classes into fewer categories if the data per class is sparse, and fitting an algorithm on the new data to see if performance is improved. Imbalanced data, where the class proportions are skewed, is not a serious concern as the algorithms used handle imbalanced data quite well. If the performance of a fitted classifier is poor, however, you may want to consider addressing this via, e.g., upsampling or downsampling. Refer to the example datasets if in doubt about the required data format. Note that clicking the Load button will reset the app and erase all uploaded data and results.

## Setting a seed
The 'Seed' numeric field is used to ensure that results are reproducible. The value you enter specifies the seed for the MATLAB random number generator. If you set the same random seed value each time you fit an algorithm to the data, it will ensure that all random processes (e.g., shuffles and partitions of the data) involved in fitting are reproduced, and the fit results will be identical. Reproducibility can be affected, however, if the app is set to utilize parallel computing (see below).

## Parallel computing
Click the 'Parallel' state button to implement parallel computing on your multicore device. It will change from light grey to dark to indicate it is on, and change back to light grey if you switch it off. The processes described below for training, tuning, and testing an algorithm can be computationally expensive and slow, therefore making use of parallel computing is advised for multicore devices. Note that this may affect the reproducibility of your results; parallel Bayesian optimization of machine learning algorithms does not always yield exactly the same results due to the nonreproducibility of parallel timing.

## Fitting an algorithm
Click the 'Run' button to begin model fitting. Two procedures will be initiated automatically: (i) [model evaluation](#model-evaluation), whereby a resampling strategy will be used to generate multiple training and test samples from the data to estimate how well a fitted algorithm will perform when confronted with new data; and (ii) a [final model version](#final-model-version) will be generated using all the data for learning.

### Model evaluation
The primary purpose in machine learning is to maximize predictive accuracy on new, unseen data. It is therefore standard practice in machine learning to use 'resampling strategies' to estimate this unknown quantity, whereby separate portions (folds) of a dataset are used for training (learning), tuning of hyperparameters (these cannot be learned from data), and testing of predictive accuracy. This provides an estimate of how well the algorithm (derived using a specific model building process) will perform when confronted with new data (generalization ability) and how much it is expected to vary in practice. This is useful for reporting purposes and to compare different algorithms and algorithm versions. In the app this all goes on behind the scenes. The user just needs to upload data and click the 'Run' button. Results on test folds are displayed in the [Train / tune /test results table](#results-table) and can be saved for later use ([Saving model results](~saving-model-results)). Once the skill of a machine learning procedure on unseen data has been estimated we are finished with resampling, all of the trained/tuned models are discarded, and the process of fitting an algorithm using all of the data begins automatically. This is known as the [final model version](#final-model-version) or 'deployment version' and is the model that will be applied in the [Projections tab](#projections-tab) to perform climate change risk assessments. For interested users the next paragraph explains the process of training, tuning, and testing via resampling in more detail; this can be skipped if desired but will help to understand the results. 

Three resampling strategies are utilized in the app. These are leave-one-out cross-validation, *k*-fold cross-validation, and the holdout method. One of these strategies will be applied automatically depending on the size of the dataset that is uploaded. Cross-validation (CV) is a useful resampling procedure when the size of the dataset is limited. CV has a single hyperparameter *k* that controls the number of subsets that a dataset is split into. Once split, each subset is given the opportunity to be used as a test set while all other subsets are combined as the training dataset. This means that *k*-fold CV involves fitting and evaluating *k* versions of an algorithm. This, in turn, provides *k* estimates of an algorithm's performance on the dataset in the [Train / tune /test results table](#results-table). Leave-one-out CV is an extreme version of *k*-fold CV where *k* is set to the number of examples in the dataset. It requires one model to be created and evaluated for each example in the dataset, and therefore has the maximum computational cost. The 'holdout' option is a simple data split where 80% of the data is used for training and tuning and 20% is 'held out' for testing the optimized algorithm, therefore *k* = 1. In the app, the selected technique is applied in a procedure known as 'nested CV' (NCV). In NCV, there are two loops of CV: an inner and an outer loop. Each training set of the outer CV is further subdivided into folds in an inner loop of CV to find the best hyperparameter values for that training set. This is achieved using 'Bayesian optimization'. Once the optimal hyperparameter values are found, the algorithm is retrained and tested in the outer loop. This is considered to be the 'gold-standard' approach in machine learning for training, tuning, and testing an algorithm, but it is computationally expensive and may be slow to run depending on the size of your dataset. [Parallel computing](#parallel-computing) can be utilized to increase execution time via the 'Parallel' state button if you have a multicore device. Automatic selection of a resampling strategy follows a commonly used approach based on the number of examples  *n* in your dataset: *n* <= 20 = leave-one-out CV, 20 < *n* <= 100 = 10-fold CV, 100 < *n* <= 10,000 = 5-fold CV, and *n* > 10,000 = 'holdout'. Note that for classification tasks the app implements 'stratified sampling' to ensure that the ratio of the observations in each class remains the same in each sample, which is useful for imbalanced datasets. Also note that prior to sampling the rows of data are randomly shuffled to ensure that samples are representative of the overall distribution of the data, which may not be the case if they have been artificially ordered (by class, for example). 
 
### Results table
The table displays the results for each iteration of the resampling strategy. There will be *k* rows of results representing *k* versions of an algorithm and *k* estimates of predictive performance on the held-out test folds. These will appear one-by-one as the app cycles through the iterations of the resampling strategy. The columns are explained as follows. 'Method' lists the ensemble aggregation algorithm that was automatically selected. 'Fold' provides the number of the test fold, *k*. The next 5 or 6 columns provide the optimized hyperparameter values for regression and classification, respectively (note that column headings for hyperparameters change according to the modelling task selected). A description of the various hyperparameters is beyond the scope of this documentation, but the interested reader can consult the [MATLAB documentation](https://www.mathworks.com/help/index.html?s_tid=CRUX_topnav). The primary metric of interest in this table for most users is the final column 'error', which is a measure of predictive accuracy on each sample of the data that was reserved for testing purposes. Results for each test sample (specific model instance) can be reported individually. Alternatively, the average will provide an estimate of the predictive skill of the entire model building process, and the standard deviation an estimate of how much the skill of the procedure is expected to vary in practice.

### Final model version
A final machine learning model is the version that is used to make predictions on new data. The model is finalized by applying the chosen machine learning procedures on all of the data. The final version is therefore produced using one final round of Bayesian optimization via resampling (non-nested CV or flat-CV) to obtain the optimal hyperparameter settings, and the resultant optimized algorithm is retrained using all the data. The details of the finalized algorithm will automatically appear in the 'Final version' table after [model evaluation](#model-evaluation) via resampling is complete. 

## Saving model results
Once the 'Save' button becomes available (after resampling and model finalization), this can be used to open up a dialogue box to save modelling results to a location of your choice. The results file is an excel workbook with 4 worksheets: 'testCases' contains the variables used and predicted values for Y in each test fold, 'performance' contains the Train / tune / test results table, 'finalModel' contains the Final version table, and 'importance' contains estimates of predictor importance derived from ``fitrensemble`` / ``fitcensemble``. These are numerical values between 0 and 1, with higher scores indicating variables that had a greater impact on making predictions.  

# Projections Tab
In this tab the finalized model is used to perform climate change risk assessments according to the scenarios of your choice.

## Climate and crop panel
Use the circular Knobs to select the future decade of interest, the season, the crop distribution (grid cells) to implement the model in, and the crop region. The crop region is used to refine the selected crop distribution to specific climatic regions of Scotland, as defined by the UK Met. Office ([UK Climate Districts Map](https://www.metoffice.gov.uk/research/climate/maps-and-data/about/districts-map)). The crop acronyms are: B = barley, BL = broadleaved tree, C = conifers, O = oats, OSR = oilseed rape, SP = seed potatoes, SF = soft fruit, WP = ware potatoes, and W = wheat. To visualise your selected crop distribution, choose 'crop' from the listbox and a map of Scotland showing the percentage of each grid cell occupied by that crop species will be automatically generated. Similarly, select a climate variable from the listbox to obtain a map of that variable for the decade and season of your choice. 

## Results panel
Click the 'Plot' to produce a boxplot of projected values for your scenario. Each boxplot is a 'super-ensemble' of projected values (12 potential future climates x *n* selected grid cells). Boxes extend from the first to the third quartile, medians are marked in each box, and whiskers extend to 1.5 times the interquartile range. You can display multiple boxplots (i.e. multiple scenarios, such as a different decades / seasons / crops etc.) together in the Projections pane. These will be numbered sequentially. For regression tasks, boxplots show projected values relative to those for the 1981-2000 baseline climatology to show the proportional response; i.e. the percentage change in the response variable relative to baseline conditions. This is just for visualisation purposes and the absolute projected values will be stored for saving. For classification tasks, stacked bar charts are used to show the distribution of projected classes, i.e., the absolute number in each class. These are not taken relative to baseline conditions. Click the 'Clear' button to clear the plot pane. Note that this will also clear the stored projections results.

## Saving projection results
Clicking the 'Save' button will open up a dialogue box to save the projection results that are plotted in the Projections pane to a location of your choice. The results file is an excel workbook with 2 worksheets: 'baseline' contains predictions from your model for UKCP18 modelled baseline conditions (1981-2000), and 'projections' contains the projected values. The column headers are numbered sequentially to match the boxplots in the Projections pane. Note that scenarios comprised of different crop selections will contain a different number of results.

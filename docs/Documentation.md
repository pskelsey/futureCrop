![leafSun](https://user-images.githubusercontent.com/32124230/107999911-9f3f1100-6fe0-11eb-9755-f48dbb82cf65.png)

# DOCUMENTATION

# Table of Contents
* [Background](#background)
  * [Climate projections](#climate-projections)
  * [Crop data](#crop-data)
  * [Example data](#example-data)
* [Basic operation](#basic-operation)
* [Model Tab](#model-tab)
  * [Modelling task](#modelling-task)
  * [Required data format and upload](#required-data-format-and-upload)
  * [Fitting an algorithm](#fitting-an-algorithm)
  ** [Sampling strategy](#sampling-strategy)
  * [Reproducible results](#reproducible-results)
  * [Parallel computing](#parallel-computing)
  * [Results table](#results-table)
  * [Final model version](#final-model-version)
  * [Saving model results](#saving-model-results)
* [Projections tab](#projections-tab)
  * [Climate and crop panel](#climate-and-crop-panel)
  * [Results panel](#results-panel)
  * [Saving projection results](#saving-projection-results)
  
# Background
The app provides a very simple front-end to cutting edge machine learning algorithms and techniques, and is bundled with gridded crop distribution and climate change data. With a few clicks a weather-dependent regression- or classification-tree ensemble can be fitted to data and applied across real crop locations to explore possible future changes under various climate change scenarios. 

### Climate projections
The app uses seasonal 12 km gridded (65 x 112 cells) climate data from the UK Met Office Climate Projections database [UKCP18](https://www.metoffice.gov.uk/research/approach/collaboration/ukcp/index) 12-member ensemble of Regional projections. The following variables are provided: temperature (deg. C), relative humidity (%), wind speed (m s<sup>-1</sup>), net surface shortwave radiation flux (W m<sup>-2</sup>), precipitation (mm d<sup>-1</sup>), total cloud (%). The data are at presented at seasonal timescales (spring, summer, autumn, and winter) for each decade spanning 2020s-2060s. UKCP18 modelled data for 1981-2000 are included as baseline values for comparison. All the climate data have full spatial and temporal coherence, enabling the assessment of multiple climatic drivers of changing hazards across multiple geographic locations. The data provide 12 equally plausible snapshots of climate change for emissions scenario RCP8.5, meaning that fitted models will produce an ensemble of 12 projected values for each grid cell, and a super-ensemble of results for each crop species distribution.

### Crop data
Polygon data defining the spatial coverage of crops and land-use types were derived from [IACS](https://ec.europa.eu/agriculture/direct-support/iacs_en) and [JACS](https://www.gov.scot/collections/june-scottish-agricultural-census/). These data cover Scotland only. The vector data were rasterised to 12 km grids matching the resolution of the climate change data. Information on the abundance (area) of crop in each grid cell is used for visualisation purposes only, and the presence-absence of selected crop species is used to narrow the climate change risk assessment to relevant grid cells. 

### Example data
Two synthetic datasets (classificationExample.xlsx, regressionExample.xlsx) have been included to allow users to investigate the various features of the app. For best results, fit a model in the Model tab and apply it in barley crop distributions using summer weather data. 

# Basic operation
*Tabs*: The app has two 'tabs' - 'Model' and 'Projections.' Click on the tab name to switch from one to the other.  
*Switches*: Drag the circluar switch to the left or right, or click in the empty space.  
*Lists*: Click on the desired option in the list.  
*Buttons*: Click them.  
*Numeric fields*: Click in the white box to change the numerical value, then hit enter or click outside the box.  
*State buttons*: Click them to toggle between two states - dark grey indicates 'on' and light grey is 'off'.  
*Knobs*: Drag the circular control around to your selection, or click on your selection.  
Note: some controls may be unavailable (greyed out) until certain operations have been performed, for example, model results cannot be saved until a model has been fitted to your data.  

# Model Tab
This is a simple front-end that allows non-modellers to easily access cutting-edge machine learning algorithms and fit them to data to create a risk model that can be applied in the [Projections tab](#projections-tab) to perform climate change risk assessments. User data must be a function of one or more of the weather variables included in the [Climate projection](#climate-projections) datasets. Inclusion of other weather variables in your datafile will produce meaningless results. The 'response variable' (dependent variable) can be any weather-dependent process involving / affecting / occurring in or around crops, e.g., crop diseases, pests, crop growth etc.    

### Modelling task
The app uses the MATLAB commands ``fitrensemble`` to fit regression tree ensembles, and ``fitcensemble`` to fit classification tree ensembles, depending on the required modelling task. This is selected using the 'Task' switch. For regression tasks the Y variable should be continuous and for classification tasks it must be categorical. If an attempt is made to upload data that do not match the selected task, a warning will be issued and the data will not be loaded. 

### Required data format and upload
To upload data for model fitting click the 'Load' button. This will open up a dialogue box that allows you to search for your file. Data files must be .xls or .xlsx. Data should be arranged in rows with columns for the different weather variables. The column header for the response (dependent) variable should be an upper case Y and the column headers for your weather variables should be all lower case: temp, rh, rain, wind, rad, cloud. It doesn't matter what order the columns are in, and you can include as many of the set of 6 weather variables as required. Missing data should be left as blank cells and these will be ignored during model fitting; do not use numerical missing data identifiers (e.g., -9999) as these will be treated as numbers. Text entries will be treated as missing data. Data for classification tasks require a little more consideration. The categorical response variable must be coded as integers and not any form of character or string array (text, alphanumeric, symbols etc.), e.g., low, medium, and high should be converted to 1, 2, 3. Integers will be treated as discrete classes by the algorithms. This is required to eliminate the risk of typos, use of different date formats etc. You ideally want >25 values of Y for each discrete class. An acceptable performance may be achieved with fewer samples per class, but <5 per class on average will result in a warning and the data will not load. Consider aggregating the classes into fewer categories if the data per class is sparse, and fitting an algorithm on the new data to see if performance is improved. Unbalanced data, where the class proportions are skewed, is not a serious concern as the algorithms used handle imbalanced data quite well. If the performance of a fitted classifier is poor, however, you may want to consider addressing this via, e.g., upsampling or downsampling. Refer to the example datasets if in doubt about the required data format. 

### Fitting an algorithm


#### Sampling strategy
Machine learning algorithms have parameters that are learned from the data (during a process known as training) and hyperparameters that affect the learning process, whose value must be set before learning begins (finding the optimal values is known as tuning). Resampling methods use different protions (referred to here as 'folds') of a dataset for training, tuning, and testing of predictive accuracy. This provides an estimate of how well the entire model building process will perform on average when confronted with new (unseen) data, and how much it is expected to vary in practice. This is useful for reporting purposes and to compare different algorithms and algorithm versions. In the app this all goes on behind the scenes. The user just needs to upload data and click the 'Run' button. Results are displayed in the [Train / tune /test results table](#results-table) and can be saved for later use ([Saving model results](~saving-model-results)). Once the skill of a machine learning procedure on unseen data has been estimated we are finished with resampling, all of the trained/tuned models are discarded, and the process of fitting an algorithm using all of the data begins automatically. This is known as the [final model version](#final-model-version) or 'deployment version' and is the model that will be applied in the [Projections tab](#projections-tab) to peform climate change risk assessments. For interested users the next paragraph explains the process of training, tuning, and testing via resampling in more detail; this can be skipped if desired but will help to understand the results. 

Three resampling strategies are utlised in the app. These are leave-one-out cross-validation, *k*-fold cross-validation, and the holdout method. One of these strategies will be applied automatically depending on the size of the dataset that is uploaded. Cross-validation (CV) is a useful resampling procedure when the size of the dataset is limited. CV has a single hyperparameter *k* that controls the number of subsets that a dataset is split into. Once split, each subset is given the opportunity to be used as a test set while all other subsets are combined as the training dataset. This means that *k*-fold CV involves fitting and evaluating *k* versions of an algorithm. This, in turn, provides *k* estimates of an algorithm's performance on the dataset in the [Train / tune /test results table](#results-table). Leave-one-out CV is an extreme version of *k*-fold CV where *k* is set to the number of examples in the dataset. It requires one model to be created and evaluated for each example in the dataset, and therefore has the maximum computational cost. The 'holdout' option is a simple data split where 80% of the data is used for training and tuning and 20% is 'held out' for testing the optimized algorithm. In the app, the selected technique is applied in a procedure known as 'nested CV' (NCV). In NCV, there are two loops of CV: an inner and an outer loop. Each training set of the outer CV is further subdivided into folds in an inner loop of CV to find the best hyperparameter values for that training set. This is achieved using 'Bayesian optimization' via MATLAB's ``bayesopt`` command.  Once the optimal hyperparameter values are found, the algorithm is retrained and tested in the outer loop. This is considered to be the 'gold-standard' approach in machine learning for training, tuning, and testing an algorithm, but it is computationally expensive and may be slow to run depending on the size of your dataset. [Parallel computing](#parallel-computing) can be utilized to increase execution time via the 'Parallel' state button if you have a multicore device. Automatic selection of a resampling strategy follows a commonly used approach based on the number of examples  *n* in your dataset: *n* <= 20 = leave-one-out CV, 20 < *n* <= 100 = 10-fold CV, 100 < *n* <= 10,000 = 5-fold CV, and *n* > 10,000 = 'holdout'. Note that for classification tasks the app implements 'stratified sampling' to ensure that the ratio of the observations in each class remains the same in each sample, which is useful for imbalanced datasets. Also note that prior to sampling the rows of data are randomly shuffled to ensure that samples are representative of the overall distribution of the data, which may not be the case if they have been artificially ordered (by class, for example). 
 
### Reproducible results
The 'Seed' numeric field specifies the seed for the MATLAB random number generator and can be used to ensure that all random shuffles and splits of the data are fully reproducible, providing reproducible results for the Model tab for the same seed value. Reproducibility can be affected, however, if the app is set to utilise parallel computing (see below).

### Parallel computing
Training, tuning, and testing an algorithm can be computationally expensive and slow, therefore making use of parallel computing is advised for multicore devices. This is achieved by depressing the 'Parallel' state button (it will change from light grey to dark grey). Parallel Bayesian optimization (tuning) does not always yield exactly reproducible results, however, due to the nonreproducibility of parallel timing.

### Results table

The primary metric of interest in this table is the 'error', which is a measure of predictive accuracy on each sample of the data that was reserved for testing purposes. Results for each test sample can be reported indivudally or averaged. 

Click the 'Run' button to begin model fitting. A progress bar will appear above the table to indicate that computations are underway. The table displays the results for each iteration of the selected sampling strategy. For 'none' and 'holdout' there will be one row and for sampling strategies prefixed by NCV there will be *k* rows of results in the table. These will appear one-by-one as the app cycles through each iteration of sampling. The columns are explained as follows. 'Fold' provides the number of the test fold (i.e., the iteration count). 'Method' provides the best estimated learner (ensemble aggregation algorithm) on the test fold. The next 5 or 6 columns provide the optimized hyperparameter values for regression and classification, respectively (note that column headings for hyperparameters change according to the modelling task selected). The final two columns provide a measure of predictive accuracy on the training fold and test fold. For regression this is the mean squared error of prediction and for classification it is the misclassification rate (proportion of cases misclassified). If 'none' is selected as the sampling strategy then the test accuracy will be NaN (not a number) as no test data were set aside. Although test accuracy is the primary metric of interest, predictive accuracy on training folds is provided for comparison. Performance is often slightly lower on test folds, but if there is a large discrepancy in training and test accuracy it indicates a poor ability of the algorithm to generalize to new, unseen data. For reporting purposes you can provide the individual results on each test fold (Test column) or the mean and SD across the test folds. If NaN appears for any hyperparameter value, this indicates that the best estimated learner does not use that hyperparameter. A description of the various ensemble aggregation algorithms (and their hyperparamters) that can be returned by ``fitrensemble`` and ``fitcensemble``is beyond the scope of this documentation, but the interested reader can consult the [MATLAB documentation](https://www.mathworks.com/help/index.html?s_tid=CRUX_topnav). 

### Final model version
The details of the finalized algorithm will automatically appear in the 'Final version' table after assessment of the model building process via resmapling is complete. If the sampling strategy selected was 'none', then default settings were used for ``fitrensemble`` / ``fitcensemble`` and the default learner has already been trained on all the available data. Otherwise, one more round of Bayesian optimization with 5-fold cross-validation is performed using to obtain the best learner and optimal hyperparameter settings, and the resultant optimized learner is retrained using all the data.

### Saving model results
Once the 'Save' button becomes available (after resampling and model finalization), this can be used to open up a dialogue box to save modelling results to a location of your choice. The results file is an excel workbook with 4 worksheets: 'testCases' contains the variables used and predicted values for Y in each test fold, 'performance' contains the Train / tune / test results table, 'finalModel' contains the Final version table, and 'importance' contains estimates of predictor importance dervied from ``fitrensemble`` / ``fitcensemble``. These are numerical values between 0 and 1, with higher scores indicating variables that had a greater impact on making predictions.  

# Projections Tab
In this tab the finalized model is used to perform climate change risk assessments according to the scanerios of your choice.

### Climate and crop panel
Use the circular Knobs to select the future decade of interest, the season, the crop distribution (grid cells) to implement the model in, and the crop region. The crop region is used to refine the selected crop distribution to specific climatic regions of Scotland, as defined by the UK Met. Office ([UK Climate Districts Map](https://www.metoffice.gov.uk/research/climate/maps-and-data/about/districts-map)). The crop acronyms are: B = barley, BL = braodleaved tree, C = conifers, O = aots, OSR = oilseed rape, SP = seed potatoes, SF = soft fruit, WP = ware potatoes, and W = wheat. To visualise your selected crop distribution, choose 'crop' from the listbox and a map of Scotland showing the percentage of each grid cell occupied by that crop species will be automatically generated. Similarly, select a climate variable from the listbox to obtain a map of that variable for the decade and season of your choice. 

### Results panel
Click the 'Plot' to produce a boxplot of projected values for your scenario. Each boxplot is a 'super-ensemble' of projected values (12 potential future climates x *n* selected grid cells). Boxes extend from the first to the third quartile, medians are marked in each box, and whiskers extend to 1.5 times the interquartile range. You can display multiple boxplots (i.e. multiple scenarios, such as a different decades / seasons / crops etc.) together in the Projections pane. These will be numbered sequentially. For regression tasks, boxplots show projected values relative to those for the 1981-2000 baseline climatology to show the proportional response; i.e. the percentage change in the response variable relative to baseline conditions. This is just for visualisation purposes and the absolute projected values will be stored for saving. For classification tasks, stacked bar charts are used to show the distribution of projected classes, i.e., the absolute number in each class. These are not taken relative to baseline conditions. Click the 'Clear' button to clear the plot pane. Note that this will also clear the stored projections results.

### Saving projection results
Clicking the 'Save' button will open up a dialogue box to save the projection results that are plotted in the Projections pane to a location of your choice. The results file is an excel workbook with 2 worksheets: 'baseline' contains predictions from your model for UKCP18 modelled baseline conditions (1981-2000), and 'projections' contains the projected values. The column headers are numbered sequentially to match the boxplots in the Projections pane. Note that scenarios comprised of different crop selections will contain a different number of results. 



